{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6SWkK0xIEGp",
        "outputId": "086826d6-6d4f-4086-a4d9-8344dec4d4fe"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from re import sub, split, M \n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaFVs4mhKx-A",
        "outputId": "1d4c5bed-3a6e-4c04-b4af-5d5686077b01"
      },
      "outputs": [],
      "source": [
        "noSentences = 0\n",
        "corpus=[]\n",
        "visited_links=[]\n",
        "file=open('text.txt', 'w')\n",
        "def scrape():\n",
        "    global corpus\n",
        "    global visited_links\n",
        "    global noSentences\n",
        "    req = requests.get(queue.pop(0))\n",
        "    html_content = req.content\n",
        "    soup = BeautifulSoup(html_content, 'html5lib')\n",
        "    news_corpus = soup.find_all('a')\n",
        "    for i in soup.find_all(\"section\", class_=\"entry-content\"):\n",
        "        list = i.findChildren(\"p\")\n",
        "        for x in list:\n",
        "            sent_list=x.get_text()\n",
        "            sentence = nltk.sent_tokenize(sent_list)\n",
        "            corpus = corpus + sentence\n",
        "            noSentences = noSentences + len(sentence)\n",
        "            #print(noSentences)\n",
        "            if(noSentences >= 10500):\n",
        "                return\n",
        "    for i in news_corpus:\n",
        "        link=i.get('href')\n",
        "        if(link is None):\n",
        "            continue\n",
        "        if((link.startswith(\"https://longreads.com/\")) and (link not in visited_links)):\n",
        "            queue.append(link)\n",
        "            visited_links.append(link)\n",
        "            #print(link)\n",
        "\n",
        "\n",
        "url = 'https://longreads.com/'\n",
        "queue = [url]\n",
        "req = requests.get(url)\n",
        "while(len(queue)>0 and noSentences<10000):\n",
        "    scrape()\n",
        "for i in corpus: \n",
        "    print(i)\n",
        "    file.write(i)\n",
        "print(noSentences)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc9OGQrVI7zQ",
        "outputId": "82256f69-e971-4d75-d011-e1dd6c8683a0"
      },
      "outputs": [],
      "source": [
        "# Word tokenization\n",
        "tokens=[]\n",
        "file=open('tokens.txt','w')\n",
        "banned = ['.',',','|','***','?','!','\\\\','\\/',':', ')', '(', '\"','—',';', '...', '“', '”', '’']\n",
        "for current in corpus:\n",
        "    current = current.strip()\n",
        "    current = sub(r'([.!?])', r'\\1 ', current)\n",
        "    current = split(r'  ',current)\n",
        "    # if not current[-1]:\n",
        "    #     del(current[-1])\n",
        "    for i in current:\n",
        "        i = nltk.word_tokenize(i)\n",
        "        for x in i:\n",
        "            if(x not in banned):\n",
        "                tokens.append(x)\n",
        "                file.write(f'{x}\\n')\n",
        "print(tokens)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixCnMhdMI8Xf",
        "outputId": "f3ccfc09-c02f-482c-f84e-2b3e5c868bbf"
      },
      "outputs": [],
      "source": [
        "#POS Tagging\n",
        "file=open('postags.txt','w')\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "for i in pos_tags:\n",
        "    print(f\"{i[0]} --> {i[1]}\\n\")\n",
        "    file.write(f\"{i[0]} --> {i[1]}\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUqXyqoWI-Wz",
        "outputId": "a0989106-6f33-44f9-c411-a03d5d0d9298"
      },
      "outputs": [],
      "source": [
        "#Remove stopwords\n",
        "tokens_without_sw = [word for word in tokens if not word.lower() in stopwords.words('english')]\n",
        "file=open(\"nostop.txt\", \"w\")\n",
        "for i in tokens_without_sw:\n",
        "    print(f\"{i}\\n\")\n",
        "    file.write(f\"{i}\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv2WKpCvcdTH",
        "outputId": "37a4f713-1505-4c0a-a5bc-d6cc31c932c0"
      },
      "outputs": [],
      "source": [
        "#Stemming\n",
        "stemmed_tokens = []\n",
        "file=open('stems.txt','w')\n",
        "for i in tokens_without_sw:\n",
        "    stemmed_tokens.append(nltk.PorterStemmer().stem(i))\n",
        "    file.write(f'{nltk.PorterStemmer().stem(i)}\\n')\n",
        "print(stemmed_tokens)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j04ISYk7cePJ",
        "outputId": "c94ec2f2-70f9-479a-e857-bd2720e5d333"
      },
      "outputs": [],
      "source": [
        "#Lemmatizing\n",
        "lemmatized_tokens = []\n",
        "file=open('lemmas.txt','w')\n",
        "for i in tokens_without_sw:\n",
        "    lemmatized_tokens.append(nltk.WordNetLemmatizer().lemmatize(i))\n",
        "    file.write(f'{nltk.WordNetLemmatizer().lemmatize(i)}\\n')\n",
        "print(lemmatized_tokens)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeE_bfsEnupA"
      },
      "source": [
        "###Token frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "1rJektkXsoVt",
        "outputId": "74b96342-bc32-4c9c-9497-4f7586e793d3"
      },
      "outputs": [],
      "source": [
        "token_counts = Counter(tokens_without_sw)\n",
        "token_counts = [[word, freq] for word,freq in sorted(token_counts.items(), key=lambda i: i[1])][::-1]\n",
        "\n",
        "print(token_counts)\n",
        "\n",
        "courses = [token_counts[i][0] for i in range(0, 22)]\n",
        "frequencies = [token_counts[i][1] for i in range(0,22)]\n",
        "print(courses)\n",
        "print(frequencies)\n",
        "fig = plt.figure(figsize=(22,10))\n",
        "plt.xlabel(\"Tokens\")\n",
        "plt.ylabel(\"No. of times it appears\")\n",
        "plt.title(\"Token Frequency\")\n",
        "plt.bar(courses, frequencies)\n",
        "plt.plot(courses,frequencies)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ErZGaVn0vV"
      },
      "source": [
        "###POS Tag Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "cnzq6PX6eUw5",
        "outputId": "c1ea6b50-5052-4ee7-fe73-a399ecf565a4"
      },
      "outputs": [],
      "source": [
        "taglist=[]\n",
        "for line in pos_tags:\n",
        "    taglist.append(line[1])\n",
        "\n",
        "postag_counts = Counter(taglist)\n",
        "postag_counts = [[word, freq] for word,freq in sorted(postag_counts.items(), key=lambda i: i[1])][::-1]\n",
        "\n",
        "print(postag_counts)\n",
        "courses = [postag_counts[i][0] for i in range(0, 10)]\n",
        "frequencies = [postag_counts[i][1] for i in range(0,10)]\n",
        "print(courses)\n",
        "print(frequencies)\n",
        "fig = plt.figure(figsize=(11,10))\n",
        "plt.xlabel(\"POS Tags\")\n",
        "plt.ylabel(\"No. of times it appears\")\n",
        "plt.title(\"POS Tag Frequency\")\n",
        "plt.bar(courses, frequencies)\n",
        "plt.plot(courses,frequencies)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ukPDm2Dn4xz"
      },
      "source": [
        "###Stem frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "qk92TWr9n8I1",
        "outputId": "15c3fca6-4b26-42d6-c639-89014912f522"
      },
      "outputs": [],
      "source": [
        "stem_counts = Counter(stemmed_tokens)\n",
        "stem_counts = [[word, freq] for word,freq in sorted(stem_counts.items(), key=lambda i: i[1])][::-1]\n",
        "\n",
        "print(stem_counts)\n",
        "\n",
        "courses = [stem_counts[i][0] for i in range(0, 10)]\n",
        "frequencies = [stem_counts[i][1] for i in range(0,10)]\n",
        "print(courses)\n",
        "print(frequencies)\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.xlabel(\"Stems\")\n",
        "plt.ylabel(\"No. of times it appears\")\n",
        "plt.title(\"Stem Frequency\")\n",
        "plt.bar(courses, frequencies)\n",
        "plt.plot(courses,frequencies)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3GYkdmyoFqw"
      },
      "source": [
        "###Lemma frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "c6pxbNfdoPZO",
        "outputId": "bdd643ea-ebe0-4a49-db8b-0d0120f3d163"
      },
      "outputs": [],
      "source": [
        "lemma_counts = Counter(lemmatized_tokens)\n",
        "\n",
        "lemma_counts = [[word, freq] for word,freq in sorted(lemma_counts.items(), key=lambda i: i[1])][::-1]\n",
        "\n",
        "print(lemma_counts)\n",
        "\n",
        "courses = [lemma_counts[i][0] for i in range(0, 10)]\n",
        "frequencies = [lemma_counts[i][1] for i in range(0,10)]\n",
        "print(courses)\n",
        "print(frequencies)\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.xlabel(\"Lemmas\")\n",
        "plt.ylabel(\"No. of times it appears\")\n",
        "plt.title(\"Lemma Frequency\")\n",
        "plt.bar(courses, frequencies)\n",
        "plt.plot(courses,frequencies)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyyIdcxctLte"
      },
      "source": [
        "#Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09k3o4m0tPJ8",
        "outputId": "2d95b899-1650-4195-d798-7529acdfb923"
      },
      "outputs": [],
      "source": [
        "token_counts = Counter(tokens_without_sw)\n",
        "token_counts = [[word, freq] for word,freq in sorted(token_counts.items(), key=lambda i: i[1])][::-1]\n",
        "# file=open('nostop.txt','r')\n",
        "# data=file.read()\n",
        "courses = [token_counts[i][0] for i in range(0, 21)]\n",
        "print(courses)\n",
        "wc = WordCloud(background_color = 'black', width = 1920, height = 1080)\n",
        "wc.generate_from_text(\" \".join(courses))\n",
        "wc.to_file('wordcloud.png')\n",
        "# file.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CL1 Mini Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
